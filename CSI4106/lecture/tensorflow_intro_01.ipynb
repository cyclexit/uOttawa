{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensors:\n",
    "Tensors can be considered as a general representation for multidimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_135753/1080951354.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# rank = axes = ndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# rank-0 tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# rank = axes = ndim\n",
    "# rank-0 tensors\n",
    "x_0 = np.array(12)\n",
    "print(x_0.shape)\n",
    "print(x_0.ndim)\n",
    "\n",
    "# rank-1 tensors\n",
    "x_1 = np.array([1, 2, 3, 4])\n",
    "print(x_1.shape)\n",
    "print(x_1.ndim)\n",
    "\n",
    "# rank-2 tensors\n",
    "x_2 = np.array([[1, 2], [3, 4]])\n",
    "print(x_2.shape)\n",
    "print(x_2.ndim)\n",
    "\n",
    "# rank-3 tensors\n",
    "x_3 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8,]]])\n",
    "print(x_3.shape)\n",
    "print(x_3.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = relu(dot(input, w) + b)\n",
    "# What does ReLU do:\n",
    "# if dot product is < 0 return 0 else output\n",
    "def relu_function(x):\n",
    "    assert len(x.shape) == 2\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    \n",
    "    return x\n",
    "\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(relu_function(x))\n",
    "\n",
    "# NOTE: using numpy\n",
    "# z = x + y\n",
    "# z = np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is broadcasting\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "\n",
    "z = np.dot(x, y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product between a vector and a matrix\n",
    "x = np.array([[1, 2, 3]])\n",
    "y = np.array([[4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "z = np.dot(x, y)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor reshaping\n",
    "x = np.array([[1, 2,], [3, 4], [5, 6]])\n",
    "print(x.shape)\n",
    "\n",
    "x = x.reshape((6, 1))\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x = x.reshape((2, 3))\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic differentiation:\n",
    "# Capability or obtaining gradients on any number of tensor operations that are differentiable\n",
    "\n",
    "# Stores tensor operations in a form of computational graph\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # NOTE: using the calculus power rule\n",
    "    # NOTE: taking the derivative means taking the slope of the function at a given point\n",
    "    y = 3 * x ** 4\n",
    "    # NOTE: using the tape to calculate the gradient of the output\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating gradients on a list of variables\n",
    "# variables are mutable objects\n",
    "w = tf.Variable(tf.random.uniform((2, 2)))\n",
    "b = tf.Variable(tf.zeros((2, )))\n",
    "x = tf.random.uniform((2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.matmul(x, w) + b\n",
    "    # gradients of y with respect to \"w\" and \"b\"\n",
    "    grad_of_y = tape.gradient(y, [w, b])\n",
    "    print(grad_of_y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
